{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer on Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an attempt to use the model released by Hu, Zhiting, et al. on other sentiments aside from positive and negative\n",
    "\n",
    "Hu, Zhiting, et al. \"Toward controlled generation of text.\" Proceedings of the 34th International Conference on Machine Learning-Volume 70. JMLR. org, 2017. Available: https://arxiv.org/pdf/1703.00955.pdf. https://github.com/asyml/texar/tree/master/examples/text_style_transfer\n",
    "\n",
    "The actual model was different from the one proposed in the paper. The actual model used as far as we could backtrack their code and explanations is as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Pretrainingjupy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the pretraining:\n",
    "The autoencoder is trained with losses from the reconstruction loss and the predicted label of the reconstruction\n",
    "\n",
    "The classifier is trained "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Trainingjupy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training:\n",
    "The classifier parameters are fixed\n",
    "The autoencoder is trained similarly, EXCEPT instead of adding some transform (the code uses a 200 vector MLP transoform) of the origin label to the latent code, what is actually added is a transform of the desired label\n",
    "\n",
    "(i.e. if 1 == positive, 0 == negative, a sentence with an attached label of 0  will have a transform of 1 concatenated to it's latent code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology and running the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the code of the simplified model provided by Hu et al.\n",
    "\n",
    "We tried extending the length of the sequences(sentences) allowed however our machine could not handle it and to be safe we just used the same length they did (21)\n",
    "\n",
    "We also removed words from the vocabulary extending past 15 characters as we found that there were some links, etc. included. Further more a lot of these words were probably outliers, that is, they were only used one time. We also removed the corresponding sentences.\n",
    "\n",
    "Preprocessing and forming of database was handled by Aerjay and the code is available in his github: https://github.com/aerjayc/formality/tree/authors\n",
    "\n",
    "## To run the code go to the /text-style-transfer folder and run python main.py --config config\n",
    "\n",
    "/texar-master/texar-master/examples/text-style-transfer\n",
    "\n",
    "We have set it up such that thet database loaded is the authors database (i.e. we replaced the contents of their files but did not rename them) and the training time runs for roughly the same as the one used by Hu et al. However we did not adjust the hyperparameters, as we lacked the time to experiment with them. Each training run takes around 10 hours on our machine (GTX 1060)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - Formality Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full training samples are available at the samples_formality folder\n",
    "val1-val220 are reconstruction attempts (pretraining)\n",
    "val221-val300 are sentiment transfer attempts (between formal and informal and vice versa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentences from the database could not even be reconstructed. We think this is because to the extreme spread in sentence variety. The vocabulary was also relatively large compared to the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/ptval219.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end result then is barely understandable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/ptval300.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results - Author Style Transfer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we decided to work on author style transfer.First, because the vocabulary would be much smaller due to the lack of informal/varied words. Second, the dataset could easily be acquired from pdfs of books. Third, we could think of two authors of the top of our heads with varying styles\n",
    "\n",
    "We used Mark Twain's The Adventures of Tom Sawyer and Jane Austen's Pride and Prejudice\n",
    "\n",
    "The text we used is in the /text-style-transfer folder labelled as MarkTwain and Jane Austen\n",
    "\n",
    "Full training samples are available at the samples_author folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As can be seen the reconstruction is much better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/tval219.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model is also able to somewhat perform a style transfer.\n",
    "\n",
    "For example it learns to replace characters names with those used by another author (tom -> jane f.). It also learns that some authors are much more likely to use certain words (i.e. waylaid for Mark Twain was replaced by simpers for Jane Austen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/tval300.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis/Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately we ran into errors running Hu et al.'s code.\n",
    "\n",
    "One we had to take out the part that printed the BLEU metric discussed in the paper. Instead we will only performed a visual/cursory analysis\n",
    "\n",
    "Two we could not run the code on Jupyter notebook, so instead we will be including two training logs at the end of this notebook. Unfortunately we did not manage to save the training log of the formality-run that was able to complete.\n",
    "\n",
    "In the github we provide the following files can be found \n",
    "\n",
    "TrainLog1 : the training log of the uncompleted formality run (the training log of the completed run was not saved)\n",
    "\n",
    "TrainLog2 : the training log of the completed author-style transfer run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Noteworthy points.\n",
    "The sentiment transfer results by Hu et al. seemingly can be reduced to mere vocabulary switches. I.e. change delicious to horrible\n",
    "\n",
    "### Below are some samples from rerunning their experiment on our machine. The same can be seen in from the samples in their github."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/huetalsamples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For formality transfer which is much than simply replacing a few words here and there (as most words don't necessarily have an informal counterpart), it seems that the model by Hu et al. is insufficient as of now.\n",
    "\n",
    "It should be noted however that we were not able to train even the autoencoder part to convergence with our given dataset.\n",
    "\n",
    "Perhaps a thing to note here is that\n",
    "\n",
    "### We seem to not have considered how hard it is to define formality/informality and how the database we used defined it.\n",
    "\n",
    "With that in mind what we did was we created our own database on a much better defined sentiment (Mark Twain vs. Jane Austen) with much more standardized vocabulary.\n",
    "\n",
    "### From this, with a much smaller database than the one used by Hu et al in their experiment. We were able to achieve a similar (visually) performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formality-informality transfer failed because it is much harder to perform via a word replacement strategy\n",
    "\n",
    "The style transfer that transcends a straight one-to-one transfer failed.\n",
    "\n",
    "However we did have a better idea of what Hu et al.â€™s SIMPLIFIED model was capable of. Roughly it seems to be able to indicate keywords which expresses sentiment and replace them with keywords that indicate another sentiment.\n",
    "\n",
    "We proved this by performing an author-style transfer of sorts.\n",
    "\n",
    "Note that we did not run the exact model they proposed in the paper. What we used was the simplified/adapted model they released\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
